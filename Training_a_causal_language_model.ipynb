{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM-FT/blob/main/Training_a_causal_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFFWmDS_aCyG"
      },
      "source": [
        "# Training a causal language model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 설치"
      ],
      "metadata": {
        "id": "iRidxJsUyu9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENzHIpE8aCyJ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 가져오기"
      ],
      "metadata": {
        "id": "73iPsAOfubNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python 코드는 GitHub과 같은 코드 저장소에서 풍부하게 제공되며, 모든 Python 저장소를 스크래핑하여 데이터셋을 생성하는 데 사용할 수 있습니다. 약 180GB에 달하는 GitHub 덤프를 사용하여 'codeparrot'이라는 이름으로 약 2천만 개의 Python 파일을 포함하는 데이터셋을 구축하고, 이를 Hugging Face Hub에 공유했습니다.\n",
        "\n",
        "따라서 transformersbook/codeparrot 데이터셋은 Hugging Face 데이터셋 허브에 호스팅되어 있는 데이터셋으로, 자연어 처리(NLP) 및 기계 학습 분야에서 사용할 수 있는 프로그래밍 코드 샘플을 포함하고 있습니다. 이 데이터셋은 프로그래밍 언어를 이해하고 생성할 수 있는 AI 모델을 개발하는 데 중점을 둡니다.\n",
        "\n",
        "그러나 전체 코퍼스에서 훈련하는 것은 시간과 계산 리소스를 많이 소모하며, 우리는 필요한 데이터셋의 부분집합을 필터링하기 위해 아래와 같은 코드를 작성할 수 있습니다."
      ],
      "metadata": {
        "id": "yOkYYgLFg-ia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_DX_KBuaCyM"
      },
      "outputs": [],
      "source": [
        "def any_keyword_in_string(string, keywords):\n",
        "    for keyword in keywords:\n",
        "        if keyword in string:\n",
        "            return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM_TrLOHaCyM"
      },
      "outputs": [],
      "source": [
        "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "example_1 = \"import numpy as np\"\n",
        "example_2 = \"import pandas as pd\"\n",
        "\n",
        "print(\n",
        "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCU5Tg4yaCyN"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "    filtered_dict = defaultdict(list)\n",
        "    total = 0\n",
        "    for sample in tqdm(iter(dataset)):\n",
        "        total += 1\n",
        "        if any_keyword_in_string(sample[\"content\"], filters):\n",
        "            for k, v in sample.items():\n",
        "                filtered_dict[k].append(v)\n",
        "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "    return Dataset.from_dict(filtered_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 함수는 데이터셋에서 키워드(pandas, sklearn, matplotlib, seaborn)와 연관된 데이터를 필터링합니다. 그러나 매우 시간소모가 크기 때문에 직접 수행하지 않겠습니다."
      ],
      "metadata": {
        "id": "PhHeTG7sq93C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umebet71aCyN"
      },
      "outputs": [],
      "source": [
        "# This cell will take a very long time to execute, so you should skip it and go to\n",
        "# the next one!\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# split = \"train\"  # \"valid\"\n",
        "# filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
        "\n",
        "# data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
        "# filtered_data = filter_streaming_dataset(data, filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드는 위에서 우리가 살펴본 필터링 이후 전체 데이터셋의 3.26%에 해당하는 8.25G의 텍스트 데이터 다운로드합니다. 몇 분을 소요합니다."
      ],
      "metadata": {
        "id": "lZ6ctjvLc52o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l0L926vaCyN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train.shuffle().select(range(500)),\n",
        "        \"valid\": ds_valid.shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "# DatasetDict({\n",
        "#     train: Dataset({\n",
        "#         features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
        "#         num_rows: 606720\n",
        "#     })\n",
        "#     valid: Dataset({\n",
        "#         features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
        "#         num_rows: 3322\n",
        "#     })\n",
        "# })\n",
        "\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 데이터셋에서 예제를 살펴보겠습니다. 각 필드의 처음 200자만 보여드리겠습니다:"
      ],
      "metadata": {
        "id": "M4MxO-0Nsi49"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkt3ik92aCyO"
      },
      "outputs": [],
      "source": [
        "for key in raw_datasets[\"train\"][5]:\n",
        "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:300]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 출력을 깔끔하게 보여줍니다. (탭과 줄바꿈 적용)\n",
        "for i, content in enumerate(raw_datasets[\"train\"][:2][\"content\"]):\n",
        "    print(f\"Content {i + 1}:\")\n",
        "    print(content)\n",
        "    print()"
      ],
      "metadata": {
        "id": "5adiA-mlKM-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 데이터를 토큰화하여 훈련에 사용할 수 있도록 합니다.\n",
        "\n",
        "우리의 목표는 주로 짧은 함수 호출을 자동 완성하는 것으로 컨텍스트 크기를 상대적으로 작게 유지할 것 입니다.이로 인해 모델을 훨씬 빠르게 훈련시킬 수 있고, 훨씬 적은 메모리가 필요합니다. 만약 응용 프로그램에서 더 많은 컨텍스트가 중요하다면(예를 들어, 함수 정의가 포함된 파일을 바탕으로 단위 테스트를 작성하게 하고 싶은 경우), 그 숫자를 증가시키는 것이 좋습니다.\n",
        "\n",
        "하지만 이것이 더 큰 GPU 메모리 사용량을 수반합니다. 일단 지금은 GPT-2나 GPT-3에서 사용된 1,024 또는 2,048 토큰 대신 128 토큰으로 컨텍스트 크기를 고정하겠습니다.\n",
        "\n",
        "대부분의 문서는 128 토큰보다 많으므로, 입력을 최대 길이로 단순히 자르는 것은 데이터셋의 큰 부분을 제거하게 됩니다. 이때 return_overflowing_tokens 옵션을 사용하면 전체 입력을 토큰화하면서 동시에 여러 청크로 나눌 수 있습니다. 또한 return_length 옵션을 사용하여 생성된 각 청크의 길이를 자동으로 반환하겠습니다. 대부분의 경우 마지막 청크는 컨텍스트 크기보다 작을 것이고, 우리는 이러한 부분을 제거할 것입니다; 패딩 문제를 피하기 위해서이며, 어차피 충분한 데이터가 있기 때문에 이 부분이 필요하지 않습니다."
      ],
      "metadata": {
        "id": "CjpeMs28tA8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0BC917VaCyO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length = 128\n",
        "#context_length는 생성할 토큰의 최대 길이를 설정\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
        "#\"huggingface-course/code-search-net-tokenizer\" 모델을 로드합니다. 이 토크나이저는 코드 검색과 관련된 작업에 최적화된 특별한 설정과 토큰화 규칙을 갖고 있습니다.\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"content\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "# raw_datasets[\"train\"][:2][\"content\"]는 훈련 데이터셋에서 처음 두 개의 샘플을 가져옵니다. 이 데이터는 'content' 키로 접근할 수 있습니다.\n",
        "# truncation=True는 토큰 길이가 max_length를 초과할 경우 뒷부분을 잘라내도록 설정합니다.\n",
        "# return_overflowing_tokens=True는 최대 길이를 초과하는 토큰들을 별도로 반환하도록 합니다. 이는 데이터가 max_length보다 길 경우 여러 부분으로 나뉘어 처리됨을 의미합니다.\n",
        "# return_length=True는 각 토큰화된 입력의 길이를 반환하도록 설정합니다.\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")\n",
        "\n",
        "#Chunk Mapping: 각 청크가 원본 데이터셋의 어느 샘플에서 왔는지를 나타냅니다. 배열의 값 0은 첫 번째 샘플에서 나온 청크들임을, 1은 두 번째 샘플에서 나온 청크들임을 나타냅니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab"
      ],
      "metadata": {
        "id": "yeWzUTCyvQwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "id": "YDLKyjJUvRlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 전체 데이터셋을 tokenize 함수로 전처리합니다."
      ],
      "metadata": {
        "id": "DcpxhC7WLR20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV32seaBaCyO"
      },
      "outputs": [],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"content\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length: #짧은 길이의 마지막 chunk는 제거될 것 입니다.\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"valid\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 초기화하기\n",
        "\n",
        "우리는 GPT-2 모델을 새롭게 초기화할 것입니다. 우리의 모델에 대해서는 작은 GPT-2 모델과 동일한 구성을 사용할 것이므로, 사전 훈련된 구성을 불러오고 토크나이저의 크기가 모델 어휘 사전 크기와 일치하는지 확인한 후, 시퀀스의 시작과 끝을 나타내는 bos (beginning of sequence) 및 eos (end of sequence) 토큰 ID를 전달할 것입니다."
      ],
      "metadata": {
        "id": "-KglVzRbugTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab"
      ],
      "metadata": {
        "id": "TMggKTGSvKvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.vocab)"
      ],
      "metadata": {
        "id": "aM9Q55Cju0ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV9BgxKYaCyO"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz7qV--_aCyO"
      },
      "outputs": [],
      "source": [
        "# 모델 생성 (config를 사용)\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 시작하기 전에, 배치를 생성할 데이터 콜레이터를 설정해야 합니다. DataCollatorForLanguageModeling는 언어 모델링을 위해 특별히 설계되었습니다. 배치를 쌓고 패딩하는 것 외에도, 이 데이터 콜레이터는 언어 모델 레이블을 생성하는 작업도 처리합니다. DataCollatorForLanguageModeling은 마스크 언어 모델링(MLM)과 인과적 언어 모델링(CLM)을 모두 지원합니다. 기본적으로는 MLM을 위한 데이터를 준비하지만, mlm=False를 설정함으로써 CLM으로 전환할 수 있습니다:"
      ],
      "metadata": {
        "id": "Jc0McA16wKQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LCqc1eaaCyP"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드를 실행해서 각 샘플이 같은 shape의 tensor로 쌓인 것을 확인하세요."
      ],
      "metadata": {
        "id": "SzIQ6d93xD9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SamDADyDaCyP"
      },
      "outputs": [],
      "source": [
        "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "레이블을 입력과 정렬하기 위해 한 칸 씩 이동시키는 작업은 모델 내부에서 일어나므로, 데이터 콜레이터는 입력을 복사하여 레이블을 생성하기만 합니다."
      ],
      "metadata": {
        "id": "nP4fNj3Vxg2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out['input_ids'][0][:10]"
      ],
      "metadata": {
        "id": "PjSAwocUOWda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out['labels'][0][:10]"
      ],
      "metadata": {
        "id": "Xyj300HxOpvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TrainingArguments](https://huggingface.co/docs/transformers/v4.39.3/en/main_classes/trainer#transformers.TrainingArguments)는 모델 학습에 대한 다양한 옵션을 설정할 수 있습니다. 배치 사이즈는 256( per_device_train_batch_size*gradient_accumulation_steps)을 사용해줍니다."
      ],
      "metadata": {
        "id": "jSH_xEirPEaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMAV3wFDaCyP"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"codeparrot-ds\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=10, #5_000\n",
        "    logging_steps=10, #5_000,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.1,\n",
        "    warmup_steps=10, #1_000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=5e-4,\n",
        "    save_steps=5_000,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "# output_dir=\"codeparrot-ds\": 훈련 중 생성된 모델과 기타 출력 파일들을 저장할 디렉토리 경로입니다.\n",
        "# per_device_train_batch_size=32: 각 디바이스(예: GPU)에 대한 훈련 배치 크기입니다.\n",
        "# per_device_eval_batch_size=32: 각 디바이스에 대한 평가 배치 크기입니다.\n",
        "# evaluation_strategy=\"steps\": 평가를 실행할 전략을 지정합니다. \"steps\"는 일정 스텝마다 평가를 실행하게 합니다.\n",
        "# eval_steps=5_000: 평가를 실행할 스텝 간격을 설정합니다.\n",
        "# logging_steps=5_000: 로깅을 실행할 스텝 간격입니다. 훈련 진행 상황을 기록합니다.\n",
        "# gradient_accumulation_steps=8: 몇 번의 훈련 스텝 후에 그래디언트를 업데이트할지 설정합니다. 이는 메모리 사용을 최적화하는 데 도움을 줄 수 있습니다.\n",
        "# num_train_epochs=1: 모델을 전체 데이터셋을 사용해 훈련할 에폭 수입니다.\n",
        "# weight_decay=0.1: 가중치 감소(Regularization)를 위한 계수입니다. 모델의 과적합을 방지하는 데 도움을 줍니다.\n",
        "# warmup_steps=1_000: 학습률 스케줄러가 본격적으로 시작되기 전에 진행할 웜업 스텝의 수입니다.\n",
        "# lr_scheduler_type=\"cosine\": 학습률 스케줄러의 유형을 지정합니다. \"cosine\"은 학습률을 코사인 함수의 형태로 점차 감소시키는 방식입니다.\n",
        "# learning_rate=5e-4: 초기 학습률 설정입니다.\n",
        "# save_steps=5_000: 모델을 저장할 스텝 간격입니다.\n",
        "# fp16=True: 16비트 부동소수점(Floating Point) 연산을 사용합니다. 이는 GPU에서의 메모리 사용량과 연산 속도를 최적화하는 데 도움을 줍니다.\n",
        "# push_to_hub=True: 훈련이 완료된 모델을 Hugging Face Hub에 자동으로 푸시합니다. 이를 통해 모델을 공유하고 버전 관리할 수 있습니다. 로그인이 필요하므로 False로 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"],\n",
        ")"
      ],
      "metadata": {
        "id": "dQsJDGIpi8nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJbq6IC7aCyP"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x6ft-9irDy0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIWDxMXoaCyL"
      },
      "source": [
        "모델을 업로드하기 위해서는 token을 받아와 huggingface hub에 로그인하고, push_to_hub 메서드로 모델을 업로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG9xsE-jaCyL"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN2d7qbcaCyP"
      },
      "outputs": [],
      "source": [
        "#trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습완료된 모델 사용"
      ],
      "metadata": {
        "id": "Lk88n2JcDgFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vLKSu3MaCyP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "pipe = pipeline(\n",
        "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", max_length=128, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ7PhRvcaCyP"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create scatter plot with x, y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72dsLk0saCyQ"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\\\n",
        "# create some data\n",
        "x = np.random.randn(100)\n",
        "y = np.random.randn(100)\n",
        "\n",
        "# create dataframe from x and y\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw5C-VYzaCyQ"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\\\n",
        "# dataframe with profession, income and name\n",
        "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
        "\n",
        "# calculate the mean income per profession\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xM7JeaO3aCyQ"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\n",
        "# import random forest regressor from scikit-learn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# fit random forest model with 300 estimators on X, y:\n",
        "\"\"\"\n",
        "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlI-A6Sgb9fQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}